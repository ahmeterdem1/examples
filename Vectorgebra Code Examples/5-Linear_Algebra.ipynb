{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from vectorgebra import *\n",
    "# VERSION 2.7.3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T21:20:07.866432Z",
     "start_time": "2023-12-27T21:20:07.828654Z"
    }
   },
   "id": "b059f02d90ded674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Algebra on Vectorgebra\n",
    "\n",
    "All sorts of linear algebra and related operations are defined on Vectorgebra, and mostly on vectorgebra.Matrix. These range from basic determinant, inverse, etc. operations to equation solving algorithms, eigenvalue related and decomposition related operations.\n",
    "\n",
    "There are many many methods defined for those, we are not going to cover all of those. But we will cover the methods that are probably going to be used most.\n",
    "\n",
    "We can start by the most basic 3 operations; determinant, inverse, transpose."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e0619e1850096ad"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-27T21:20:07.866708Z",
     "start_time": "2023-12-27T21:20:07.835906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[ 2, 0, 1]\n",
      "[ 3, 3, 3]\n",
      "[ 2, 5, 4]\n",
      "\n",
      "Determinant of the matrix: 3.0\n",
      "\n",
      "Inverse of the matrix:\n",
      "[ -0.04827072023700316, 0.5870669852426267  , -0.3771223564289387 ]\n",
      "[ -0.6782925149779264 , 0.5007133750041447  , -0.13498296073669735]\n",
      "[ 0.8280752864829879  , -0.8695978166810783 , 0.5785418434261905  ]\n",
      "\n",
      "Check if the inverse is correct:\n",
      "[ 0.7315338460089815 , 0.3045361538041751 , -0.1757028694316869]\n",
      "[ 0.3045361538041753 , 0.654547630697079  , 0.19930957878166344]\n",
      "[ -0.1757028694316869, 0.1993095787816639 , 0.8850078571633979 ]\n",
      "\n",
      "Transpose of the matrix:\n",
      "[ 2, 3, 2]\n",
      "[ 0, 3, 5]\n",
      "[ 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "m = Matrix.randMint(3, 3, 0, 5, False)\n",
    "print(f\"Original matrix:\\n{m}\\n\")\n",
    "print(f\"Determinant of the matrix: {m.determinant()}\\n\")\n",
    "m_inverse = m.inverse(decimal=False)\n",
    "print(f\"Inverse of the matrix:\\n{m_inverse}\\n\")\n",
    "print(f\"Check if the inverse is correct:\\n{m * m_inverse}\\n\") # Check out the %error.\n",
    "# Output of above should be an identity matrix since A * A-1 = I\n",
    "\n",
    "print(f\"Transpose of the matrix:\\n{m.transpose()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inversion\n",
    "\n",
    "You wil see that inverse is not quite correct. vectorgebra.Matrix.inverse() method is a sophisticated method that requires multiple specific arguments to be used correctly. decimal.Decimal choice that is written in the above code is one of them. \n",
    "\n",
    "Other than that, firstly, there are 4 different inversion methods in vectorgebra. Those are given to the function via \"method\" argument. Possible choices are \"analytic\", \"iterative\", \"gauss\", \"neumann\". Iterative solution is the default method that uses the Newton method. To use it with higher accuracy, you need to probably increase the \"resolution\" arguments value. This defines the number of iterations that are made during the calculation of the Newton method.\n",
    "\n",
    "Analytic methods use is discouraged for higher dimensional matrices than 5. It is however the fastest method for very low dimensional matrices. This method uses the coefficient method to calculate the adjoint matrix and then the inverse. The result of this method is _always_ exact and true (Except maybe for some very ill conditioned matrices that would have values that are very very close to zero. And even then just set \"decimal=True\".). \n",
    "\n",
    "The Neumann method only works for properly conditioned matrices. If you work on very specific cases you can use it and it is fast at its job. For more information, see the [github](https://github.com/ahmeterdem1/Vector) page.\n",
    "\n",
    "The \"gauss\" method is the Gauss-Jordan elimination method, and it is the fastest among all. It should be used for very high dimensional matrices as it is very effective at that case. This method accepts parameters \"lowlimit\" and \"highlimit\". You may need to tune these values to manage the accuracy of the gauss method. This method is heavily dependent on divisions and because of that there may be floating point errors. Even decimal.Decimal class is sometimes unable to overcome those errors. In that case, you lower the \"highlimit\" and increase the \"lowlimit\". Divisions that result in values smaller than \"lowlimit\" are assumed zero, divisions that result in values higher than \"highlimit\" are also assumed zero. This is because when the denominator is too small but not zero, values blow up. \"highlimit\" literally limits those errors. Even though all the required tuning (default values will work just fine for most of the cases), this method is blazing fast and works very efficiently. And also the results are very very accurate. \n",
    "\n",
    "### Determinant\n",
    "\n",
    "Determinant also can be done via 2 different methods. One of these is \"analytic\" which is the basis of the analytic inversion method. This method is very slow for more than 5 dimensional matrices, however the value is always exact. The other method is \"echelon\", which is the default method. This method first reduces the matrix into its echelon form, then multiplies the diagonals since it would be a triangular matrix. This method is very fast and should be used for most of the cases. For 2, 3 and 4 dimensional matrices you may opt to use the analytic method for better accuracy (and it is faster for those matrices)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22a190ec192099a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Least Squares\n",
    "\n",
    "There are different methods to solve equations or find the best fitting result in vectorgebra.Matrix. We will only look at the least squares method for a system of equations. Let's first create a randomized dataset to work on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6933e32bb23c5332"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resultant vector is: [2.822594475427586, 2.045436408073498]\n",
      "The error vector is: [0.1774055245724142, 0.045436408073498136]\n"
     ]
    }
   ],
   "source": [
    "line = lambda x: 2*x + 3 # We will generate points around this line\n",
    "\n",
    "x_list = [k for k in range(10)]\n",
    "y_list = [line(x + random.uniform(-0.5, 0.5)) for x in x_list] #We will deviate from the line randomly.\n",
    "v_list = [Vector(1, x) for x in x_list]\n",
    "\n",
    "y = Vector(*y_list)\n",
    "A = Matrix(*v_list)\n",
    "\n",
    "result = A.least_squares(y, resolution=40, decimal=False) # Solve for Ax=y\n",
    "\n",
    "print(f\"The resultant vector is: {result}\")\n",
    "print(f\"The error vector is: {(Vector(3, 2) - result).map(abs)}\") # \"abs\" is defined in vectorgebra"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T21:20:07.881515Z",
     "start_time": "2023-12-27T21:20:07.854287Z"
    }
   },
   "id": "9c79953a36dd778c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Least squares method utilizes inverses of matrices. Every argument that vectorgebra.Matrix.inverse() has is also accepted by vectorgebra.Matrix.least_squares(). The method returns a vector that has as values $b_{0}$ and $b_{1}$ in order.\n",
    "\n",
    "Least squares is a good example for entry to production code usages in vectorgebra. Above example has a little bit of everything. We start by defining an objective, then we generate our simulation data. After that we structure our simulation data into vectors and matrices and then we finally utilize .least_squares() to get to a solution.\n",
    "\n",
    "At the end we finish by the calculation of the error vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe16a9e237c1ba87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at one last method. THe echelon form. With this method, I guess we have completed the top 5 of linear algebra. This method only accepts square matrices. To row reduce non-square matrices, you may append all zero vectors at the end of the matrix until you reach a square matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f287c734ae256d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[ 4, 4, 2]\n",
      "[ 5, 5, 1]\n",
      "[ 2, 3, 3]\n",
      "\n",
      "Reduced matrix:\n",
      "[ 4.0 , 3.0 , 0.0 ]\n",
      "[ 0.0 , 0.75, 0.0 ]\n",
      "[ 0.0 , 1.0 , 2.0 ]\n",
      "\n",
      "Compare the determinants of both (via analytic method to not cheat):\n",
      "\n",
      "Original determinant:6\n",
      "Determinant after reduction:6.0\n"
     ]
    }
   ],
   "source": [
    "m = Matrix.randMint(3, 3, 0, 5, False)\n",
    "print(f\"Original matrix:\\n{m}\\n\")\n",
    "m_reduce = m.echelon()\n",
    "print(f\"Reduced matrix:\\n{m_reduce}\\n\")\n",
    "\n",
    "print(\"Compare the determinants of both (via analytic method to not cheat):\\n\")\n",
    "print(f\"Original determinant:{m.determinant(choice='analytic')}\")\n",
    "print(f\"Determinant after reduction:{m_reduce.determinant(choice='analytic')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T21:20:07.882017Z",
     "start_time": "2023-12-27T21:20:07.858962Z"
    }
   },
   "id": "f6f7060029c3221e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Always keep in mind that row reduction based operations like echelon form, echelon determinant, Gauss-Jordan inversion, etc. are prone to floating point errors. However, these errors are rare and the related algorithms are probably the fastest of the library."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7f90c961fdfd95a"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T21:20:07.882568Z",
     "start_time": "2023-12-27T21:20:07.861123Z"
    }
   },
   "id": "c1b5d4c3b06d51c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
